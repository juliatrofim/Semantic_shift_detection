{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_micro = pd.read_csv('diachrony_for_russian/datasets/micro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(WORD, year):\n",
    "    word_sent = []\n",
    "    with open('data/proc_data_{}.txt'.format(str(year)), 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            try:\n",
    "                if WORD in l.split():\n",
    "                    word_sent.append(l)\n",
    "            except:\n",
    "                print(w, l)\n",
    "          \n",
    "    return word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(WORD, word_sent, model):\n",
    "    emb = []\n",
    "    for s in word_sent:\n",
    "        try:\n",
    "            idx = s.split().index(WORD)\n",
    "            input_ids = tf.constant(tokenizer.encode(s.split()))[None, :]\n",
    "            outputs = model(input_ids)[0]\n",
    "            embed = outputs[:, idx + 1, :] \n",
    "            emb.append(embed.numpy().reshape(-1))\n",
    "        except:   \n",
    "            pass\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for y in range(2000, 2015):\n",
    "    model1 = model2\n",
    "    model2 = TFBertModel.from_pretrained('models/model_' + str(y + 1), from_pt=True)\n",
    "    words = df_micro.loc[df_micro['BASE_YEAR'] == y]\n",
    "    print(y)\n",
    "    for idx, row in words.iterrows():\n",
    "        WORD = row['WORD'][:-4]\n",
    "        year = row['BASE_YEAR']\n",
    "        label = row['GROUND_TRUTH']\n",
    "        if WORD + str(year) in data_fix:\n",
    "            continue\n",
    "\n",
    "        word_sent = get_sent(WORD, year)\n",
    "        c1 = len(word_sent)\n",
    "        word_sent = word_sent[:100]\n",
    "        print('Find {} word usage'.format(len(word_sent)))\n",
    "        emb = get_emb(WORD, word_sent, model1)\n",
    "        print('Make {} word embeddings'.format(len(emb)))\n",
    "\n",
    "        word_sent1 = get_sent(WORD, year + 1)\n",
    "        c2 = len(word_sent1)\n",
    "        word_sent1 = word_sent1[:100]\n",
    "        print('Find {} word usage'.format(len(word_sent1)))\n",
    "        emb1 = get_emb(WORD, word_sent1, model2)\n",
    "        print('Make {} word embeddings'.format(len(emb1)))\n",
    "        all_p = []\n",
    "        all_p.extend(emb)\n",
    "        all_p.extend(emb1)\n",
    "        if len(emb) == 0 or len(emb1) == 0:\n",
    "            continue\n",
    "\n",
    "        kmeans = KMeans(n_clusters=K, random_state=0).fit(all_p)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        p1 = []\n",
    "        for i in range(K):\n",
    "            l = kmeans.labels_[:len(emb)]\n",
    "            p1.append(np.sum(l == i) / len(l))\n",
    "        p2 = []\n",
    "        for i in range(K):\n",
    "            l = kmeans.labels_[len(emb):]\n",
    "            p2.append(np.sum(l == i) / len(l))\n",
    "            \n",
    "        data_fix[WORD + str(year)] = [p1, p2, c1, c2, label]\n",
    "    \n",
    "    del model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_micro.iterrows():\n",
    "    WORD = row['WORD'][:-4]\n",
    "    year = row['BASE_YEAR']\n",
    "    if WORD + str(year) in data_fix:\n",
    "        p1 = data_fix[WORD + str(year)][0]\n",
    "        p2 = data_fix[WORD + str(year)][1]\n",
    "        c1 = data_fix[WORD + str(year)][2]\n",
    "        c2 = data_fix[WORD + str(year)][3]\n",
    "        l = data_fix[WORD + str(year)][4]\n",
    "        X.append([np.max(np.square(np.array(p1) - np.array(p2))),\n",
    "                  distance.jensenshannon(p1, p2), c1, c2\n",
    "                 ])\n",
    "        y.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score as f1\n",
    "res_score = []\n",
    "for i in range(9):\n",
    "    n = 30\n",
    "    x_train = X[:i * n] + X[(i + 1) * n:]\n",
    "    y_train = y[:i * n] + y[(i + 1) * n:]\n",
    "    x_test = X[i * n:(i + 1) * n]\n",
    "    y_test = y[i * n:(i + 1) * n]\n",
    "    \n",
    "    clf = LogisticRegression(random_state=42, max_iter=10000, class_weight='balanced')\n",
    "    clf.fit(np.array(x_train), y_train)\n",
    "    pred = clf.predict(np.array(x_test))\n",
    "    res_score.append(f1(pred, y_test, average='macro'))\n",
    "\n",
    "print(np.mean(res_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score as f1\n",
    "res_score = []\n",
    "for i in range(9):\n",
    "    n = 30\n",
    "    x_train = X[:i * n] + X[(i + 1) * n:]\n",
    "    y_train = y[:i * n] + y[(i + 1) * n:]\n",
    "    x_test = X[i * n:(i + 1) * n]\n",
    "    y_test = y[i * n:(i + 1) * n]\n",
    "    \n",
    "    clf = LogisticRegression(random_state=42, max_iter=10000)\n",
    "    clf.fit(np.array(x_train), y_train)\n",
    "    pred = clf.predict(np.array(x_test))\n",
    "    res_score.append(f1(pred, y_test, average='macro'))\n",
    "\n",
    "print(np.mean(res_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
